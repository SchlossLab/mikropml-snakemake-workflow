import os
import pandas as pd
from snakemake.utils import Paramspace

configfile: "config/default.yml"

MEM_PER_GB = 1024

dataset_filename = config['dataset-csv']
dataset = config["dataset-name"]
ncores = config["ncores"]
ml_methods = config["ml-methods"]
kfold = config["kfold"]
outcome_colname = config["outcome-colname"]

nseeds = config["nseeds"]
start_seed = 100
seeds = range(start_seed, start_seed + nseeds)

hyperparams = config["hyperparams"] if "hyperparams" in config else None
find_feature_importance = config["find-feature-importance"]

ignore_keys = ['dataset-csv', 'outcome-colname', 'hyperparams', 'find-feature-importance', 'nseeds', 'ncores']
for k in ignore_keys:
    config.pop(k)

paramspace = Paramspace(pd.DataFrame.from_dict(config), param_sep = "_")
print('paramspace.wildcard_pattern:\t', paramspace.wildcard_pattern)
print('paramspace.instance_patterns:\t', [i for i in paramspace.instance_patterns])

include: "rules/learn.smk"
include: "rules/combine.smk"
include: "rules/plot.smk"
include: "rules/example-report.smk"


rule targets:
    input:
        f"report_{dataset}.md",


rule render_report:
    input:
        R="workflow/scripts/render.R",
        Rmd="report.Rmd",
        perf_plot=rules.plot_performance.output.plot,
        feat_plot="figures/{dataset}/feature_importance.png",
        hp_plot=expand(
            "figures/{{dataset}}/hp_performance_{method}.png", method=ml_methods
        ),
        bench_plot=rules.plot_benchmarks.output.plot,
    output:
        doc="report_{dataset}.md",
    log:
        "log/{dataset}/render_report.txt",
    params:
        dataset=dataset,
        nseeds=nseeds,
        ml_methods=ml_methods,
        ncores=ncores,
        kfold=kfold,
    conda:
        "envs/mikropml.yml"
    script:
        "scripts/render.R"
